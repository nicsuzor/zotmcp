# @package _global_
# Zotero Vectorization Pipeline for ZotMCP
#
# Usage: python -m buttermilk.pipeline.runner --config-path conf --config-name vectorize

defaults:
  - zotero
  - _self_

run:
  mode: pipeline

bm:
  _target_: buttermilk._core.bm_init.BM
  session_info:
    project_name: zotmcp
    job: vectorize_zotero
    cache_dir: .cache
    template_paths:
      - templates

storage:
  zotero_vectors:
    # Just change the persist_directory to point to the upstream authoritative versoin
    # So that any changes we make will be synchronised.
    persist_directory: "gs://prosocial-dev/data/zotero-prosocial-fulltext/files"

infrastructure:
  clouds:
    - type: gcp
      _target_: buttermilk._core.cloud_config.GCPConfig
      project_id: prosocial-443205
      region: us-central1
      location: us-central1
      storage_bucket: prosocial-dev

  logging:
    type: local
    verbose: true

vectoriser:
  _target_: buttermilk.data.vector.ChromaDBEmbeddings
  persist_directory: ${storage.zotero_vectors.persist_directory}
  collection_name: ${storage.zotero_vectors.collection_name}
  embedding_model: ${storage.zotero_vectors.embedding_model}
  dimensionality: ${storage.zotero_vectors.dimensionality}
  concurrency: 10
  sync_batch_size: 50
  deduplication_strategy: record_id  # Skip existing records before downloading
  enable_record_cache: true

pipeline:
  _target_: buttermilk.pipeline.PipelineOrchestrator
  pipeline_name: zotero_vectorization
  concurrency: 5
  max_records: 50000

  # Source: Zotero library with deduplication
  source:
    _target_: buttermilk.libs.zotero.ZotDownloader
    library: ${oc.env:ZOTERO_LIBRARY_ID}
    save_dir: /home/nic/src/writing/projects/zotmcp/.cache/zotero/items
    local: ${oc.env:ZOTERO_LOCAL,false}
    download_concurrency: 8
    vector_store: ${vectoriser}  # Enable early deduplication

  processors:
    # 1. Generate citations using LLM (optional but recommended)
    - _target_: buttermilk.tools.citator.Citator
      model: gemini25flash
      template: citation

    # 2. Chunk text semantically
    - _target_: buttermilk.data.vector.SemanticSplitter
      chunk_size: 1000
      chunk_overlap: 250

    # 3. Embed and upload to ChromaDB
    - ${vectoriser}

